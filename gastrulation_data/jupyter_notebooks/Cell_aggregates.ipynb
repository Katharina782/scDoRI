{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating cell aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create 500 cell aggreagates of 100 cells each. To create these groupings, we will randomly sample 500 cell from the latent space embedding. Using a nearest neighbor search we will then find the 100 nearest neighbors of these cells. Any group of cells which has an overlap >80% with any of the previously created groups will be removed. These cell aggregates can then be used for downstream correlation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numba import jit, njit\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the anndata object which contains the latent space embedding\n",
    "adata = scvi.data.read_h5ad(\"gpu_trained_20_dim/anndata_object\")\n",
    "\n",
    "# read in ArchR gene expression matrix\n",
    "\n",
    "\n",
    "# read in ArchR gene score matrix\n",
    "\n",
    "# subset the dataset to contain only cells which we also find in the ArchR gene expression matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in gene_expression matrix from ArchR\n",
    "archr_gene_expr = scvi.data.read_h5ad(\"ArchR_gene_expr.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in gene_scores based on scATAC-seq from ArchR\n",
    "archr_gene_scores = pd.read_csv(\"archr_gene_scores_table.csv\", sep = \" \", header=0)\n",
    "#archr_gene_scores = scvi.data.read_h5ad(\"ArchR_gene_scores.h5ad\")\n",
    "#archr_gene_scores_p2g = scvi.data.read_h5ad(\"ArchR_gene_scores_p2g.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E8.5_rep1#TTACGTTTCTGGCATG-1</th>\n",
       "      <th>E8.5_rep1#TCCTCTAAGTCCTTCA-1</th>\n",
       "      <th>E8.5_rep1#TATCCAGCACAGACTC-1</th>\n",
       "      <th>E8.5_rep1#GAGAACCAGACACTTA-1</th>\n",
       "      <th>E8.5_rep1#TCAAGTATCTTAGCGG-1</th>\n",
       "      <th>E8.5_rep1#CCCTTAATCTCACAAA-1</th>\n",
       "      <th>E8.5_rep1#TGACCAAGTTGGTGAC-1</th>\n",
       "      <th>E8.5_rep1#TGATGACTCAAGGACA-1</th>\n",
       "      <th>E8.5_rep1#GGTAAGGGTTTACGTC-1</th>\n",
       "      <th>E8.5_rep1#CTGAATATCATCCTCA-1</th>\n",
       "      <th>...</th>\n",
       "      <th>E8.0_rep2#GACATTATCTATCGCC-1</th>\n",
       "      <th>E8.0_rep2#TCGTAATCATTCAGCA-1</th>\n",
       "      <th>E8.0_rep2#CGCAATAGTAGCTAAT-1</th>\n",
       "      <th>E8.0_rep2#TTGTGAGGTCAGGCCA-1</th>\n",
       "      <th>E8.0_rep2#TGCACCTTCATAAGCC-1</th>\n",
       "      <th>E8.0_rep2#TCATACTTCAAGACTC-1</th>\n",
       "      <th>E8.0_rep2#TGCACACCAATTAGCT-1</th>\n",
       "      <th>E8.0_rep2#GGCAAGCCAGCCTTGG-1</th>\n",
       "      <th>E8.0_rep2#AGTTACATCGTGCTAG-1</th>\n",
       "      <th>E8.0_rep2#GCTAATATCAAACCTA-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Xkr4</th>\n",
       "      <td>0.669</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.874</td>\n",
       "      <td>1.952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rp1</th>\n",
       "      <td>0.492</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sox17</th>\n",
       "      <td>0.732</td>\n",
       "      <td>0.341</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.540</td>\n",
       "      <td>1.197</td>\n",
       "      <td>0.440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrpl15</th>\n",
       "      <td>0.433</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lypla1</th>\n",
       "      <td>0.409</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hccs</th>\n",
       "      <td>0.218</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gm15246</th>\n",
       "      <td>0.209</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mid1</th>\n",
       "      <td>0.473</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933400A11Rik</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asmt</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24333 rows Ã— 45986 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               E8.5_rep1#TTACGTTTCTGGCATG-1  E8.5_rep1#TCCTCTAAGTCCTTCA-1  \\\n",
       "Xkr4                                  0.669                         0.342   \n",
       "Rp1                                   0.492                         0.179   \n",
       "Sox17                                 0.732                         0.341   \n",
       "Mrpl15                                0.433                         0.374   \n",
       "Lypla1                                0.409                         0.063   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                  0.218                         0.111   \n",
       "Gm15246                               0.209                         0.196   \n",
       "Mid1                                  0.473                         0.463   \n",
       "4933400A11Rik                         0.212                         0.165   \n",
       "Asmt                                  0.000                         0.000   \n",
       "\n",
       "               E8.5_rep1#TATCCAGCACAGACTC-1  E8.5_rep1#GAGAACCAGACACTTA-1  \\\n",
       "Xkr4                                  0.533                         0.495   \n",
       "Rp1                                   0.279                         0.305   \n",
       "Sox17                                 1.093                         0.958   \n",
       "Mrpl15                                0.598                         0.513   \n",
       "Lypla1                                0.335                         0.274   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                  0.353                         0.208   \n",
       "Gm15246                               0.189                         0.189   \n",
       "Mid1                                  0.239                         0.284   \n",
       "4933400A11Rik                         0.077                         0.191   \n",
       "Asmt                                  0.000                         0.000   \n",
       "\n",
       "               E8.5_rep1#TCAAGTATCTTAGCGG-1  E8.5_rep1#CCCTTAATCTCACAAA-1  \\\n",
       "Xkr4                                  0.283                         0.496   \n",
       "Rp1                                   0.258                         0.066   \n",
       "Sox17                                 0.386                         0.564   \n",
       "Mrpl15                                0.365                         0.433   \n",
       "Lypla1                                0.246                         0.181   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                  0.379                         0.178   \n",
       "Gm15246                               0.313                         0.000   \n",
       "Mid1                                  0.448                         0.876   \n",
       "4933400A11Rik                         0.236                         0.499   \n",
       "Asmt                                  0.000                         0.000   \n",
       "\n",
       "               E8.5_rep1#TGACCAAGTTGGTGAC-1  E8.5_rep1#TGATGACTCAAGGACA-1  \\\n",
       "Xkr4                                  0.052                         0.277   \n",
       "Rp1                                   0.058                         0.190   \n",
       "Sox17                                 0.175                         0.540   \n",
       "Mrpl15                                0.244                         0.637   \n",
       "Lypla1                                0.255                         0.346   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                  0.064                         0.000   \n",
       "Gm15246                               0.069                         0.112   \n",
       "Mid1                                  0.426                         0.521   \n",
       "4933400A11Rik                         0.379                         0.168   \n",
       "Asmt                                  0.000                         0.000   \n",
       "\n",
       "               E8.5_rep1#GGTAAGGGTTTACGTC-1  E8.5_rep1#CTGAATATCATCCTCA-1  \\\n",
       "Xkr4                                  0.560                         0.378   \n",
       "Rp1                                   0.160                         0.229   \n",
       "Sox17                                 1.197                         0.440   \n",
       "Mrpl15                                0.274                         0.595   \n",
       "Lypla1                                0.745                         0.369   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                  0.065                         0.000   \n",
       "Gm15246                               0.019                         0.091   \n",
       "Mid1                                  0.667                         0.393   \n",
       "4933400A11Rik                         0.603                         0.127   \n",
       "Asmt                                  0.000                         0.000   \n",
       "\n",
       "               ...  E8.0_rep2#GACATTATCTATCGCC-1  \\\n",
       "Xkr4           ...                           0.0   \n",
       "Rp1            ...                           0.0   \n",
       "Sox17          ...                           0.0   \n",
       "Mrpl15         ...                           0.0   \n",
       "Lypla1         ...                           0.0   \n",
       "...            ...                           ...   \n",
       "Hccs           ...                           0.0   \n",
       "Gm15246        ...                           0.0   \n",
       "Mid1           ...                           0.0   \n",
       "4933400A11Rik  ...                           0.0   \n",
       "Asmt           ...                           0.0   \n",
       "\n",
       "               E8.0_rep2#TCGTAATCATTCAGCA-1  E8.0_rep2#CGCAATAGTAGCTAAT-1  \\\n",
       "Xkr4                                  1.874                         1.952   \n",
       "Rp1                                   0.000                         0.807   \n",
       "Sox17                                 0.000                         0.000   \n",
       "Mrpl15                                0.000                         0.000   \n",
       "Lypla1                                0.000                         0.000   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                  0.000                         0.000   \n",
       "Gm15246                               0.000                         0.000   \n",
       "Mid1                                  0.000                         0.000   \n",
       "4933400A11Rik                         0.000                         0.000   \n",
       "Asmt                                  0.000                         0.000   \n",
       "\n",
       "               E8.0_rep2#TTGTGAGGTCAGGCCA-1  E8.0_rep2#TGCACCTTCATAAGCC-1  \\\n",
       "Xkr4                                    0.0                           0.0   \n",
       "Rp1                                     0.0                           0.0   \n",
       "Sox17                                   0.0                           0.0   \n",
       "Mrpl15                                  0.0                           0.0   \n",
       "Lypla1                                  0.0                           0.0   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                    0.0                           0.0   \n",
       "Gm15246                                 0.0                           0.0   \n",
       "Mid1                                    0.0                           0.0   \n",
       "4933400A11Rik                           0.0                           0.0   \n",
       "Asmt                                    0.0                           0.0   \n",
       "\n",
       "               E8.0_rep2#TCATACTTCAAGACTC-1  E8.0_rep2#TGCACACCAATTAGCT-1  \\\n",
       "Xkr4                                    0.0                         0.000   \n",
       "Rp1                                     0.0                         0.000   \n",
       "Sox17                                   0.0                         0.000   \n",
       "Mrpl15                                  0.0                         0.000   \n",
       "Lypla1                                  0.0                         0.000   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                    0.0                         0.000   \n",
       "Gm15246                                 0.0                         0.000   \n",
       "Mid1                                    0.0                         1.954   \n",
       "4933400A11Rik                           0.0                         0.000   \n",
       "Asmt                                    0.0                         0.000   \n",
       "\n",
       "               E8.0_rep2#GGCAAGCCAGCCTTGG-1  E8.0_rep2#AGTTACATCGTGCTAG-1  \\\n",
       "Xkr4                                    0.0                           0.0   \n",
       "Rp1                                     0.0                           0.0   \n",
       "Sox17                                   0.0                           0.0   \n",
       "Mrpl15                                  0.0                           0.0   \n",
       "Lypla1                                  0.0                           0.0   \n",
       "...                                     ...                           ...   \n",
       "Hccs                                    0.0                           0.0   \n",
       "Gm15246                                 0.0                           0.0   \n",
       "Mid1                                    0.0                           0.0   \n",
       "4933400A11Rik                           0.0                           0.0   \n",
       "Asmt                                    0.0                           0.0   \n",
       "\n",
       "               E8.0_rep2#GCTAATATCAAACCTA-1  \n",
       "Xkr4                                    0.0  \n",
       "Rp1                                     0.0  \n",
       "Sox17                                   0.0  \n",
       "Mrpl15                                  0.0  \n",
       "Lypla1                                  0.0  \n",
       "...                                     ...  \n",
       "Hccs                                    0.0  \n",
       "Gm15246                                 0.0  \n",
       "Mid1                                    0.0  \n",
       "4933400A11Rik                           0.0  \n",
       "Asmt                                    0.0  \n",
       "\n",
       "[24333 rows x 45986 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archr_gene_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset gene scores matrix to only contain genes for which we also have gene expression information\n",
    "scores = archr_gene_scores[archr_gene_scores.index.isin(archr_gene_expr.var[\"name\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mat = scores.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.14 GiB for an array with shape (546058225, 2) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10984/4085356741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores_mat_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/omics/groups/OE0533/internal/katharina/scvi2/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     84\u001b[0m                                  \"\".format(self.format)) from e\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Read matrix dimensions given, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/omics/groups/OE0533/internal/katharina/scvi2/lib/python3.7/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    187\u001b[0m                                          (shape, self._shape))\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_canonical_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 8.14 GiB for an array with shape (546058225, 2) and data type int64"
     ]
    }
   ],
   "source": [
    "scores_mat_sparse = sparse.csr_matrix(scores_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores_mat_sparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10984/2486867285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mscores_mat_sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores_mat_sparse' is not defined"
     ]
    }
   ],
   "source": [
    "del scores_mat_sparse\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/20928136/input-and-output-numpy-arrays-to-h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('gene_scores.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'archr_gene_expr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10984/2719669472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0marchr_gene_expr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0marchr_gene_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'archr_gene_expr' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del archr_gene_expr\n",
    "del archr_gene_scores\n",
    "gc.collect()\n",
    "del scores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del scores\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.63 GiB for an array with shape (22273, 45986) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10984/1340435786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gene_scores'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscores_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/omics/groups/OE0533/internal/katharina/scvi2/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/omics/groups/OE0533/internal/katharina/scvi2/lib/python3.7/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_for_new_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecified_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Validate shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/omics/groups/OE0533/internal/katharina/scvi2/lib/python3.7/site-packages/h5py/_hl/base.py\u001b[0m in \u001b[0;36marray_for_new_object\u001b[0;34m(data, specified_dtype)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mas_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguess_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# In most cases, this does nothing. But if data was already an array,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 7.63 GiB for an array with shape (22273, 45986) and data type float64"
     ]
    }
   ],
   "source": [
    "h5f.create_dataset('gene_scores', data=scores_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class File in module h5py._hl.files:\n",
      "\n",
      "class File(h5py._hl.group.Group)\n",
      " |  File(name, mode='r', driver=None, libver=None, userblock_size=None, swmr=False, rdcc_nslots=None, rdcc_nbytes=None, rdcc_w0=None, track_order=None, fs_strategy=None, fs_persist=False, fs_threshold=1, fs_page_size=None, page_buf_size=None, min_meta_keep=0, min_raw_keep=0, locking=None, **kwds)\n",
      " |  \n",
      " |  Represents an HDF5 file.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      File\n",
      " |      h5py._hl.group.Group\n",
      " |      h5py._hl.base.HLObject\n",
      " |      h5py._hl.base.CommonStateObject\n",
      " |      h5py._hl.base.MutableMappingHDF5\n",
      " |      h5py._hl.base.MappingHDF5\n",
      " |      collections.abc.MutableMapping\n",
      " |      collections.abc.Mapping\n",
      " |      collections.abc.Collection\n",
      " |      collections.abc.Sized\n",
      " |      collections.abc.Iterable\n",
      " |      collections.abc.Container\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |  \n",
      " |  __init__(self, name, mode='r', driver=None, libver=None, userblock_size=None, swmr=False, rdcc_nslots=None, rdcc_nbytes=None, rdcc_w0=None, track_order=None, fs_strategy=None, fs_persist=False, fs_threshold=1, fs_page_size=None, page_buf_size=None, min_meta_keep=0, min_raw_keep=0, locking=None, **kwds)\n",
      " |      Create a new file object.\n",
      " |      \n",
      " |      See the h5py user guide for a detailed explanation of the options.\n",
      " |      \n",
      " |      name\n",
      " |          Name of the file on disk, or file-like object.  Note: for files\n",
      " |          created with the 'core' driver, HDF5 still requires this be\n",
      " |          non-empty.\n",
      " |      mode\n",
      " |          r        Readonly, file must exist (default)\n",
      " |          r+       Read/write, file must exist\n",
      " |          w        Create file, truncate if exists\n",
      " |          w- or x  Create file, fail if exists\n",
      " |          a        Read/write if exists, create otherwise\n",
      " |      driver\n",
      " |          Name of the driver to use.  Legal values are None (default,\n",
      " |          recommended), 'core', 'sec2', 'stdio', 'mpio', 'ros3'.\n",
      " |      libver\n",
      " |          Library version bounds.  Supported values: 'earliest', 'v108',\n",
      " |          'v110', 'v112'  and 'latest'. The 'v108', 'v110' and 'v112'\n",
      " |          options can only be specified with the HDF5 1.10.2 library or later.\n",
      " |      userblock_size\n",
      " |          Desired size of user block.  Only allowed when creating a new\n",
      " |          file (mode w, w- or x).\n",
      " |      swmr\n",
      " |          Open the file in SWMR read mode. Only used when mode = 'r'.\n",
      " |      rdcc_nbytes\n",
      " |          Total size of the raw data chunk cache in bytes. The default size\n",
      " |          is 1024**2 (1 MB) per dataset.\n",
      " |      rdcc_w0\n",
      " |          The chunk preemption policy for all datasets.  This must be\n",
      " |          between 0 and 1 inclusive and indicates the weighting according to\n",
      " |          which chunks which have been fully read or written are penalized\n",
      " |          when determining which chunks to flush from cache.  A value of 0\n",
      " |          means fully read or written chunks are treated no differently than\n",
      " |          other chunks (the preemption is strictly LRU) while a value of 1\n",
      " |          means fully read or written chunks are always preempted before\n",
      " |          other chunks.  If your application only reads or writes data once,\n",
      " |          this can be safely set to 1.  Otherwise, this should be set lower\n",
      " |          depending on how often you re-read or re-write the same data.  The\n",
      " |          default value is 0.75.\n",
      " |      rdcc_nslots\n",
      " |          The number of chunk slots in the raw data chunk cache for this\n",
      " |          file. Increasing this value reduces the number of cache collisions,\n",
      " |          but slightly increases the memory used. Due to the hashing\n",
      " |          strategy, this value should ideally be a prime number. As a rule of\n",
      " |          thumb, this value should be at least 10 times the number of chunks\n",
      " |          that can fit in rdcc_nbytes bytes. For maximum performance, this\n",
      " |          value should be set approximately 100 times that number of\n",
      " |          chunks. The default value is 521.\n",
      " |      track_order\n",
      " |          Track dataset/group/attribute creation order under root group\n",
      " |          if True. If None use global default h5.get_config().track_order.\n",
      " |      fs_strategy\n",
      " |          The file space handling strategy to be used.  Only allowed when\n",
      " |          creating a new file (mode w, w- or x).  Defined as:\n",
      " |          \"fsm\"        FSM, Aggregators, VFD\n",
      " |          \"page\"       Paged FSM, VFD\n",
      " |          \"aggregate\"  Aggregators, VFD\n",
      " |          \"none\"       VFD\n",
      " |          If None use HDF5 defaults.\n",
      " |      fs_page_size\n",
      " |          File space page size in bytes. Only used when fs_strategy=\"page\". If\n",
      " |          None use the HDF5 default (4096 bytes).\n",
      " |      fs_persist\n",
      " |          A boolean value to indicate whether free space should be persistent\n",
      " |          or not.  Only allowed when creating a new file.  The default value\n",
      " |          is False.\n",
      " |      fs_threshold\n",
      " |          The smallest free-space section size that the free space manager\n",
      " |          will track.  Only allowed when creating a new file.  The default\n",
      " |          value is 1.\n",
      " |      page_buf_size\n",
      " |          Page buffer size in bytes. Only allowed for HDF5 files created with\n",
      " |          fs_strategy=\"page\". Must be a power of two value and greater or\n",
      " |          equal than the file space page size when creating the file. It is\n",
      " |          not used by default.\n",
      " |      min_meta_keep\n",
      " |          Minimum percentage of metadata to keep in the page buffer before\n",
      " |          allowing pages containing metadata to be evicted. Applicable only if\n",
      " |          page_buf_size is set. Default value is zero.\n",
      " |      min_raw_keep\n",
      " |          Minimum percentage of raw data to keep in the page buffer before\n",
      " |          allowing pages containing raw data to be evicted. Applicable only if\n",
      " |          page_buf_size is set. Default value is zero.\n",
      " |      locking\n",
      " |          The file locking behavior. Defined as:\n",
      " |          False (or \"false\")  Disable file locking\n",
      " |          True (or \"true\")    Enable file locking\n",
      " |          \"best-effort\"       Enable file locking but ignore some errors\n",
      " |          None                Use HDF5 defaults\n",
      " |          Warning: The HDF5_USE_FILE_LOCKING environment variable can override\n",
      " |          this parameter.\n",
      " |          Only available with HDF5 >= 1.12.1 or 1.10.x >= 1.10.7.\n",
      " |      Additional keywords\n",
      " |          Passed on to the selected file driver.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close the file.  All open objects become invalid\n",
      " |  \n",
      " |  flush(self)\n",
      " |      Tell the HDF5 library to flush its buffers.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  attrs\n",
      " |      Attributes attached to this object\n",
      " |  \n",
      " |  driver\n",
      " |      Low-level HDF5 file driver used to open file\n",
      " |  \n",
      " |  filename\n",
      " |      File name on disk\n",
      " |  \n",
      " |  libver\n",
      " |      File format version bounds (2-tuple: low, high)\n",
      " |  \n",
      " |  mode\n",
      " |      Python mode used to open file\n",
      " |  \n",
      " |  swmr_mode\n",
      " |      Controls single-writer multiple-reader mode\n",
      " |  \n",
      " |  userblock_size\n",
      " |      User block size (in bytes)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h5py._hl.group.Group:\n",
      " |  \n",
      " |  __contains__(self, name)\n",
      " |      Test if a member name exists\n",
      " |  \n",
      " |  __delitem__(self, name)\n",
      " |      Delete (unlink) an item from this group.\n",
      " |  \n",
      " |  __getitem__(self, name)\n",
      " |      Open an object in the file\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate over member names\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Number of members attached to this group\n",
      " |  \n",
      " |  __reversed__(self)\n",
      " |      Iterate over member names in reverse order.\n",
      " |  \n",
      " |  __setitem__(self, name, obj)\n",
      " |      Add an object to the group.  The name must not already be in use.\n",
      " |      \n",
      " |      The action taken depends on the type of object assigned:\n",
      " |      \n",
      " |      Named HDF5 object (Dataset, Group, Datatype)\n",
      " |          A hard link is created at \"name\" which points to the\n",
      " |          given object.\n",
      " |      \n",
      " |      SoftLink or ExternalLink\n",
      " |          Create the corresponding link.\n",
      " |      \n",
      " |      Numpy ndarray\n",
      " |          The array is converted to a dataset object, with default\n",
      " |          settings (contiguous storage, etc.).\n",
      " |      \n",
      " |      Numpy dtype\n",
      " |          Commit a copy of the datatype as a named datatype in the file.\n",
      " |      \n",
      " |      Anything else\n",
      " |          Attempt to convert it to an ndarray and store it.  Scalar\n",
      " |          values are stored as scalar datasets. Raise ValueError if we\n",
      " |          can't understand the resulting array dtype.\n",
      " |  \n",
      " |  build_virtual_dataset(self, name, shape, dtype, maxshape=None, fillvalue=None)\n",
      " |      Assemble a virtual dataset in this group.\n",
      " |      \n",
      " |      This is used as a context manager::\n",
      " |      \n",
      " |          with f.build_virtual_dataset('virt', (10, 1000), np.uint32) as layout:\n",
      " |              layout[0] = h5py.VirtualSource('foo.h5', 'data', (1000,))\n",
      " |      \n",
      " |      name\n",
      " |          (str) Name of the new dataset\n",
      " |      shape\n",
      " |          (tuple) Shape of the dataset\n",
      " |      dtype\n",
      " |          A numpy dtype for data read from the virtual dataset\n",
      " |      maxshape\n",
      " |          (tuple, optional) Maximum dimensions if the dataset can grow.\n",
      " |          Use None for unlimited dimensions.\n",
      " |      fillvalue\n",
      " |          The value used where no data is available.\n",
      " |  \n",
      " |  copy(self, source, dest, name=None, shallow=False, expand_soft=False, expand_external=False, expand_refs=False, without_attrs=False)\n",
      " |      Copy an object or group.\n",
      " |      \n",
      " |       The source can be a path, Group, Dataset, or Datatype object.  The\n",
      " |       destination can be either a path or a Group object.  The source and\n",
      " |       destinations need not be in the same file.\n",
      " |      \n",
      " |       If the source is a Group object, all objects contained in that group\n",
      " |       will be copied recursively.\n",
      " |      \n",
      " |       When the destination is a Group object, by default the target will\n",
      " |       be created in that group with its current name (basename of obj.name).\n",
      " |       You can override that by setting \"name\" to a string.\n",
      " |      \n",
      " |       There are various options which all default to \"False\":\n",
      " |      \n",
      " |        - shallow: copy only immediate members of a group.\n",
      " |      \n",
      " |        - expand_soft: expand soft links into new objects.\n",
      " |      \n",
      " |        - expand_external: expand external links into new objects.\n",
      " |      \n",
      " |        - expand_refs: copy objects that are pointed to by references.\n",
      " |      \n",
      " |        - without_attrs: copy object without copying attributes.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |       >>> f = File('myfile.hdf5', 'w')\n",
      " |       >>> f.create_group(\"MyGroup\")\n",
      " |       >>> list(f.keys())\n",
      " |       ['MyGroup']\n",
      " |       >>> f.copy('MyGroup', 'MyCopy')\n",
      " |       >>> list(f.keys())\n",
      " |       ['MyGroup', 'MyCopy']\n",
      " |  \n",
      " |  create_dataset(self, name, shape=None, dtype=None, data=None, **kwds)\n",
      " |      Create a new HDF5 dataset\n",
      " |      \n",
      " |      name\n",
      " |          Name of the dataset (absolute or relative).  Provide None to make\n",
      " |          an anonymous dataset.\n",
      " |      shape\n",
      " |          Dataset shape.  Use \"()\" for scalar datasets.  Required if \"data\"\n",
      " |          isn't provided.\n",
      " |      dtype\n",
      " |          Numpy dtype or string.  If omitted, dtype('f') will be used.\n",
      " |          Required if \"data\" isn't provided; otherwise, overrides data\n",
      " |          array's dtype.\n",
      " |      data\n",
      " |          Provide data to initialize the dataset.  If used, you can omit\n",
      " |          shape and dtype arguments.\n",
      " |      \n",
      " |      Keyword-only arguments:\n",
      " |      \n",
      " |      chunks\n",
      " |          (Tuple or int) Chunk shape, or True to enable auto-chunking. Integers can\n",
      " |          be used for 1D shape.\n",
      " |      \n",
      " |      maxshape\n",
      " |          (Tuple or int) Make the dataset resizable up to this shape. Use None for\n",
      " |          axes you want to be unlimited. Integers can be used for 1D shape.\n",
      " |      compression\n",
      " |          (String or int) Compression strategy.  Legal values are 'gzip',\n",
      " |          'szip', 'lzf'.  If an integer in range(10), this indicates gzip\n",
      " |          compression level. Otherwise, an integer indicates the number of a\n",
      " |          dynamically loaded compression filter.\n",
      " |      compression_opts\n",
      " |          Compression settings.  This is an integer for gzip, 2-tuple for\n",
      " |          szip, etc. If specifying a dynamically loaded compression filter\n",
      " |          number, this must be a tuple of values.\n",
      " |      scaleoffset\n",
      " |          (Integer) Enable scale/offset filter for (usually) lossy\n",
      " |          compression of integer or floating-point data. For integer\n",
      " |          data, the value of scaleoffset is the number of bits to\n",
      " |          retain (pass 0 to let HDF5 determine the minimum number of\n",
      " |          bits necessary for lossless compression). For floating point\n",
      " |          data, scaleoffset is the number of digits after the decimal\n",
      " |          place to retain; stored values thus have absolute error\n",
      " |          less than 0.5*10**(-scaleoffset).\n",
      " |      shuffle\n",
      " |          (T/F) Enable shuffle filter.\n",
      " |      fletcher32\n",
      " |          (T/F) Enable fletcher32 error detection. Not permitted in\n",
      " |          conjunction with the scale/offset filter.\n",
      " |      fillvalue\n",
      " |          (Scalar) Use this value for uninitialized parts of the dataset.\n",
      " |      track_times\n",
      " |          (T/F) Enable dataset creation timestamps.\n",
      " |      track_order\n",
      " |          (T/F) Track attribute creation order if True. If omitted use\n",
      " |          global default h5.get_config().track_order.\n",
      " |      external\n",
      " |          (Iterable of tuples) Sets the external storage property, thus\n",
      " |          designating that the dataset will be stored in one or more\n",
      " |          non-HDF5 files external to the HDF5 file.  Adds each tuple\n",
      " |          of (name, offset, size) to the dataset's list of external files.\n",
      " |          Each name must be a str, bytes, or os.PathLike; each offset and\n",
      " |          size, an integer.  If only a name is given instead of an iterable\n",
      " |          of tuples, it is equivalent to [(name, 0, h5py.h5f.UNLIMITED)].\n",
      " |      allow_unknown_filter\n",
      " |          (T/F) Do not check that the requested filter is available for use.\n",
      " |          This should only be used with ``write_direct_chunk``, where the caller\n",
      " |          compresses the data before handing it to h5py.\n",
      " |  \n",
      " |  create_dataset_like(self, name, other, **kwupdate)\n",
      " |      Create a dataset similar to `other`.\n",
      " |      \n",
      " |      name\n",
      " |          Name of the dataset (absolute or relative).  Provide None to make\n",
      " |          an anonymous dataset.\n",
      " |      other\n",
      " |          The dataset which the new dataset should mimic. All properties, such\n",
      " |          as shape, dtype, chunking, ... will be taken from it, but no data\n",
      " |          or attributes are being copied.\n",
      " |      \n",
      " |      Any dataset keywords (see create_dataset) may be provided, including\n",
      " |      shape and dtype, in which case the provided values take precedence over\n",
      " |      those from `other`.\n",
      " |  \n",
      " |  create_group(self, name, track_order=None)\n",
      " |      Create and return a new subgroup.\n",
      " |      \n",
      " |      Name may be absolute or relative.  Fails if the target name already\n",
      " |      exists.\n",
      " |      \n",
      " |      track_order\n",
      " |          Track dataset/group/attribute creation order under this group\n",
      " |          if True. If None use global default h5.get_config().track_order.\n",
      " |  \n",
      " |  create_virtual_dataset(self, name, layout, fillvalue=None)\n",
      " |      Create a new virtual dataset in this group.\n",
      " |      \n",
      " |      See virtual datasets in the docs for more information.\n",
      " |      \n",
      " |      name\n",
      " |          (str) Name of the new dataset\n",
      " |      \n",
      " |      layout\n",
      " |          (VirtualLayout) Defines the sources for the virtual dataset\n",
      " |      \n",
      " |      fillvalue\n",
      " |          The value to use where there is no data.\n",
      " |  \n",
      " |  get(self, name, default=None, getclass=False, getlink=False)\n",
      " |      Retrieve an item or other information.\n",
      " |      \n",
      " |      \"name\" given only:\n",
      " |          Return the item, or \"default\" if it doesn't exist\n",
      " |      \n",
      " |      \"getclass\" is True:\n",
      " |          Return the class of object (Group, Dataset, etc.), or \"default\"\n",
      " |          if nothing with that name exists\n",
      " |      \n",
      " |      \"getlink\" is True:\n",
      " |          Return HardLink, SoftLink or ExternalLink instances.  Return\n",
      " |          \"default\" if nothing with that name exists.\n",
      " |      \n",
      " |      \"getlink\" and \"getclass\" are True:\n",
      " |          Return HardLink, SoftLink and ExternalLink classes.  Return\n",
      " |          \"default\" if nothing with that name exists.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> cls = group.get('foo', getclass=True)\n",
      " |      >>> if cls == SoftLink:\n",
      " |  \n",
      " |  move(self, source, dest)\n",
      " |      Move a link to a new location in the file.\n",
      " |      \n",
      " |      If \"source\" is a hard link, this effectively renames the object.  If\n",
      " |      \"source\" is a soft or external link, the link itself is moved, with its\n",
      " |      value unmodified.\n",
      " |  \n",
      " |  require_dataset(self, name, shape, dtype, exact=False, **kwds)\n",
      " |      Open a dataset, creating it if it doesn't exist.\n",
      " |      \n",
      " |      If keyword \"exact\" is False (default), an existing dataset must have\n",
      " |      the same shape and a conversion-compatible dtype to be returned.  If\n",
      " |      True, the shape and dtype must match exactly.\n",
      " |      \n",
      " |      Other dataset keywords (see create_dataset) may be provided, but are\n",
      " |      only used if a new dataset is to be created.\n",
      " |      \n",
      " |      Raises TypeError if an incompatible object already exists, or if the\n",
      " |      shape or dtype don't match according to the above rules.\n",
      " |  \n",
      " |  require_group(self, name)\n",
      " |      Return a group, creating it if it doesn't exist.\n",
      " |      \n",
      " |      TypeError is raised if something with that name already exists that\n",
      " |      isn't a group.\n",
      " |  \n",
      " |  visit(self, func)\n",
      " |      Recursively visit all names in this group and subgroups (HDF5 1.8).\n",
      " |      \n",
      " |      You supply a callable (function, method or callable object); it\n",
      " |      will be called exactly once for each link in this group and every\n",
      " |      group below it. Your callable must conform to the signature:\n",
      " |      \n",
      " |          func(<member name>) => <None or return value>\n",
      " |      \n",
      " |      Returning None continues iteration, returning anything else stops\n",
      " |      and immediately returns that value from the visit method.  No\n",
      " |      particular order of iteration within groups is guaranteed.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> # List the entire contents of the file\n",
      " |      >>> f = File(\"foo.hdf5\")\n",
      " |      >>> list_of_names = []\n",
      " |      >>> f.visit(list_of_names.append)\n",
      " |  \n",
      " |  visititems(self, func)\n",
      " |      Recursively visit names and objects in this group (HDF5 1.8).\n",
      " |      \n",
      " |      You supply a callable (function, method or callable object); it\n",
      " |      will be called exactly once for each link in this group and every\n",
      " |      group below it. Your callable must conform to the signature:\n",
      " |      \n",
      " |          func(<member name>, <object>) => <None or return value>\n",
      " |      \n",
      " |      Returning None continues iteration, returning anything else stops\n",
      " |      and immediately returns that value from the visit method.  No\n",
      " |      particular order of iteration within groups is guaranteed.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      # Get a list of all datasets in the file\n",
      " |      >>> mylist = []\n",
      " |      >>> def func(name, obj):\n",
      " |      ...     if isinstance(obj, Dataset):\n",
      " |      ...         mylist.append(name)\n",
      " |      ...\n",
      " |      >>> f = File('foo.hdf5')\n",
      " |      >>> f.visititems(func)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h5py._hl.base.HLObject:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |      Disable pickle.\n",
      " |      \n",
      " |      Handles for HDF5 objects can't be reliably deserialised, because the\n",
      " |      recipient may not have access to the same files. So we do this to\n",
      " |      fail early.\n",
      " |      \n",
      " |      If you really want to pickle h5py objects and can live with some\n",
      " |      limitations, look at the h5pickle project on PyPI.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h5py._hl.base.HLObject:\n",
      " |  \n",
      " |  file\n",
      " |      Return a File instance associated with this object\n",
      " |  \n",
      " |  id\n",
      " |      Low-level identifier appropriate for this object\n",
      " |  \n",
      " |  name\n",
      " |      Return the full name of this object.  None if anonymous.\n",
      " |  \n",
      " |  parent\n",
      " |      Return the parent group of this object.\n",
      " |      \n",
      " |      This is always equivalent to obj.file[posixpath.dirname(obj.name)].\n",
      " |      ValueError if this object is anonymous.\n",
      " |  \n",
      " |  ref\n",
      " |      An (opaque) HDF5 reference to this object\n",
      " |  \n",
      " |  regionref\n",
      " |      Create a region reference (Datasets only).\n",
      " |      \n",
      " |      The syntax is regionref[<slices>]. For example, dset.regionref[...]\n",
      " |      creates a region reference in which the whole dataset is selected.\n",
      " |      \n",
      " |      Can also be used to determine the shape of the referenced dataset\n",
      " |      (via .shape property), or the shape of the selection (via the\n",
      " |      .selection property).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h5py._hl.base.CommonStateObject:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h5py._hl.base.MappingHDF5:\n",
      " |  \n",
      " |  items(self)\n",
      " |      Get a view object on member items\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Get a view object on member names\n",
      " |  \n",
      " |  values(self)\n",
      " |      Get a view object on member objects\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.abc.MutableMapping:\n",
      " |  \n",
      " |  clear(self)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  pop(self, key, default=<object object at 0x7f501d5c2130>)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised.\n",
      " |  \n",
      " |  popitem(self)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair\n",
      " |      as a 2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  update(*args, **kwds)\n",
      " |      D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n",
      " |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
      " |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
      " |      In either case, this is followed by: for k, v in F.items(): D[k] = v\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Collection:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(h5py.File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#archr_gene_scores_p2g = scvi.data.read_h5ad(\"ArchR_gene_scores_p2g.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset anndata object to contain only overlapping cells\n",
    "latent_adata = adata[archr_gene_expr.obs.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latent_adata.obsm[\"X_scVI\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample cells\n",
    "\n",
    "Since we have 45,991 cells in our dataset, we will sample 1000 cell aggregates of 50 cells each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell_aggregates:\n",
    "    def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_aggregates = 1000 \n",
    "# from all cells, randomly sample 500\n",
    "sample_cells = np.random.choice(latent.shape[0], n_aggregates, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we want to find the 100 nearest neighbor for each sampled cell. We will restrict the nearest neighbors to only come from the same celltype. Since some celltypes contain less than 100 cells we will have to simply use all cells of this celltype and group them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = latent_adata.obs\n",
    "# create column for cell names\n",
    "metadata[\"cells\"] = metadata.index\n",
    "# create index for cells\n",
    "metadata[\"idx\"] = np.arange(len(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm=\"ball_tree\").fit(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for only the sampled cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the nearest neighbors to only contain the sampled cells\n",
    "filt_dist = distances[sample_cells, ]\n",
    "filt_ind = indices[sample_cells, ]\n",
    "print(filt_dist.shape, filt_ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def check_overlap(mtx):\n",
    "    nrow = mtx.shape[0]\n",
    "    ncol = mtx.shape[1]\n",
    "    # create an array to store whether a cell passed the overlap check\n",
    "    # all entries are initially False\n",
    "    considered_cells = np.zeros(nrow).astype(np.bool8)\n",
    "    \n",
    "    # loop over each cell and cosider to add it to the set of cell aggregates\n",
    "    for i in range(nrow):\n",
    "        check = True\n",
    "        # loop over all previous aggregates \n",
    "        for comp in np.where(considered_cells)[0]:\n",
    "            # get the number of cells which overlap between the neighborhood of the cell we would like to add and the neighborhoud of previous cell \"comp\"\n",
    "            intersect = np.intersect1d(mtx[i, :], mtx[comp, :])\n",
    "            # for each comparison between current cell i which we would like to add and previous cell which we are comparing to\n",
    "            # compute the percentage of overlap\n",
    "            if (len(intersect) / ncol) > 0.8: # if the intersection is above 0.8, we do not consider it\n",
    "                check = False\n",
    "                break\n",
    "        if check:\n",
    "            considered_cells[i] = True\n",
    "    # get indices\n",
    "    return np.arange(start=0, stop=nrow, step=1)[considered_cells]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_keep = check_overlap(filt_ind) # the first row corresponds to the sampled cell itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the neigbors for cells which passed the overlap check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates_keep = filt_ind[idx_keep, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot cells which pass overlap filtering and not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_df = metadata[metadata.idx.isin(filt_ind[:, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_df[\"pass_overlap\"] = np.where(subset_df[\"idx\"].isin(test[:,0]), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x = \"celltype\", y = \"pass_overlap\", hue = \"pass_overlap\", data = subset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only keep cells of same celltype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_celltypes(mat, metadata):\n",
    "    groups={}\n",
    "    # check whether a cell aggregate contains cells of other celltypes and remove then\n",
    "    for n, i in enumerate(sample_cells):\n",
    "        # get celltype information of sampled cell\n",
    "        celltype_test_cell = metadata.iloc[i][\"celltype\"]\n",
    "        # get indices of cells which are in the neighborhood \n",
    "        neighbor_cells = mat[mat[:, 0] == i, :]\n",
    "        # get cells which are of the same celltype, vector includes the sampled cell itself\n",
    "        keep = np.array(metadata[(metadata.idx.isin(neighbor_cells.flatten())) & (metadata.celltype == celltype_test_cell)][\"idx\"])\n",
    "        # keep only aggregates which contain at least 10 cells after removing non-matching celltypes\n",
    "        if keep.shape[0] > 10:\n",
    "            groups[i] = keep\n",
    "        else:\n",
    "            continue\n",
    "    return groups\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = latent_adata.obs\n",
    "# create column for cell names\n",
    "metadata[\"cells\"] = metadata.index\n",
    "# create index for cells\n",
    "metadata[\"idx\"] = np.arange(len(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = filter_celltypes(aggregates_keep, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate expression/scores/accessibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_mat = archr_gene_expr.X\n",
    "rna_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_mat[0:10, 0:10].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement aggregation with numba -> convert to csr matrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregates(mat, groups):\n",
    "    # initialize matrix to store average gene expression for each cell aggregate\n",
    "    # the matrix has dimensions genes x cell aggregates\n",
    "    rna_agg = np.zeros((mat.shape[1],len(groups)))\n",
    "    # for each cell aggregate calculate the average gene expression for each gene\n",
    "    for i, g in enumerate(groups):\n",
    "        rna_agg[:, i] = mat[groups[g], :].mean(axis=0).flatten()\n",
    "    return sparse.csr_matrix(rna_agg) # return sparse matrix genes x cell aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene expression\n",
    "rna_agg = create_aggregates(rna_mat, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mat.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene scores\n",
    "score_agg = create_aggregates(scores_mat.T, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_agg[1:50, 1:50].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute row-wise correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rna_agg.copy()\n",
    "print(A.shape)\n",
    "print(np.mean(A, axis=1).shape)\n",
    "A -= np.mean(A, axis=1)\n",
    "print(A.shape)\n",
    "print(np.std(A, axis=1).shape)\n",
    "print(np.std(A, axis=1))\n",
    "#A /= np.std(A, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[np.where(np.std(A, axis=1) == 0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rna_agg.copy()\n",
    "A.shape\n",
    "A -= np.mean(A, axis=1)\n",
    "A.shape\n",
    "A /= np.std(A, axis=1)\n",
    "B = score_agg.copy()\n",
    "\n",
    "B -= np.mean(B, axis=1)\n",
    "B /= np.std(B, axis=1)\n",
    "\n",
    "corr = np.mean(A*B, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rna_agg.copy()\n",
    "B = score_agg.copy()\n",
    "cA = A - A.mean(axis=1)\n",
    "cB = B - B.mean(axis=1)\n",
    "print(cA.shape)\n",
    "print(np.square(cA))\n",
    "#print((cA**2).shape)\n",
    "sA = np.sqrt((np.square(cA)).mean(axis=1))\n",
    "sB = np.sqrt((cB**2).mean(axis=1))\n",
    "corr = (cA*cB).mean(axis=1) / sA*sB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_idx = filt_ind[indices[sampled_cells, 0] == test, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_idx.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype = metadata.iloc[test][\"celltype\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = np.array(metadata[(metadata.idx.isin(aggregate_idx.flatten())) & (metadata.celltype == celltype)][\"idx\"])\n",
    "subset\n",
    "#subset[subset[\"celltype\"] == metadata.iloc[test][\"celltype\"]][\"idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_aggregates = 500\n",
    "n_cells_per_aggregate = 100\n",
    "group_mat = np.zeros([n_aggregates, n_cells_per_aggregate])\n",
    "celltype_list = []\n",
    "\n",
    "for n, i in enumerate(sample_cells):\n",
    "    test_cell = sample_cells[n]\n",
    "    # get celltype information for the cell\n",
    "    celltype_test_cell = metadata.iloc[test_cell][\"celltype\"]\n",
    "    celltype_list.append(celltype_test_cell)\n",
    "\n",
    "    # get all other cells which are of the same celltype and convert to numpy array\n",
    "    same_celltype = np.array(metadata[metadata[\"celltype\"] == celltype_test_cell][\"idx\"])\n",
    "    \n",
    "    # First we will check if there are more than 100 cells of this celltype\n",
    "    if same_celltype.shape[0] >= n_cells_per_aggregate:\n",
    "        #print(\"There are more than 100 cells of this celltype present\")\n",
    "        # from the connectivity matrix get only the columns which correspond to cells of the same celltype\n",
    "        subset_conn = connectivities[test_cell, same_celltype]\n",
    "        subset_dist = distances[test_cell, same_celltype]\n",
    "        # get the 100 cells with the smallest distance\n",
    "        top_hundred = np.argpartition(subset_dist.todense(), n_cells_per_aggregate)[:, :n_cells_per_aggregate]\n",
    "\n",
    "        # subset the dataframe to get only cells which are nearest neighbors \n",
    "        group_idx = top_hundred.reshape([n_cells_per_aggregate,1]).flatten()\n",
    "        group_cells = np.array(metadata[metadata[\"idx\"].isin(top_hundred.tolist()[0])][\"cells\"])\n",
    "\n",
    "    else:\n",
    "        print(f\"The number of cells for {celltype_test_cell} is below 100\")\n",
    "        group_idx = np.array(metadata[metadata[\"celltype\"] == celltype_test_cell][\"idx\"]).flatten()\n",
    "        group_cells = np.array(metadata[metadata[\"celltype\"] == celltype_test_cell][\"cells\"])\n",
    "    #print(group_idx.shape)\n",
    "    # add the cells to the matrix at position i, to create group i\n",
    "    group_mat[n, : group_cells.shape[0]] = group_idx\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(celltype_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adata.obs[adata.obs[\"celltype\"] == \"PGC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(celltype_list)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(celltype_list)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.celltype.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cell = sample_cells[0]\n",
    "\n",
    "# get celltype information for the cell\n",
    "celltype_test_cell = metadata.iloc[test_cell][\"celltype\"]\n",
    "\n",
    "# get all other cells which are of the same celltype and convert to numpy array\n",
    "same_celltype = np.array(metadata[metadata[\"celltype\"] == celltype_test_cell][\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will check if there are more than 100 cells of this celltype\n",
    "if same_celltype.shape[0] >= 100:\n",
    "    print(\"There are more than 100 cells of this celltype present\")\n",
    "    # from the connectivity matrix get only the columns which correspond to cells of the same celltype\n",
    "    subset_conn = connectivities[test_cell, same_celltype]\n",
    "    subset_dist = distances[test_cell, same_celltype]\n",
    "    # get the 100 cells with the smallest distance\n",
    "    top_hundred = np.argpartition(subset_dist.todense(), 100)[:, :100]\n",
    "\n",
    "    # subset the dataframe to get only cells which are nearest neighbors \n",
    "    group_idx = top_hundred.reshape([100,1])\n",
    "    group_cells = np.array(metadata[metadata[\"idx\"].isin(top_hundred.tolist()[0])][\"cells\"])\n",
    "\n",
    "else:\n",
    "    print(\"The number of cells for this celltype is below 100\")\n",
    "    group_idx = np.array(metadata[metadata[\"celltype\"] == celltype_test_cell][\"idx\"])\n",
    "    group_cells = np.array(metadata[metadata[\"celltype\"] == celltype_test_cell][\"cells\"]\n",
    "                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the connectivity matrix get only the columns which correspond to cells of the same celltype\n",
    "subset_conn = connectivities[test_cell, same_celltype]\n",
    "subset_dist = distances[test_cell, same_celltype]\n",
    "# get the 100 cells with the smallest distance\n",
    "top_hundred = np.argpartition(subset_dist.todense(), 100)[:, :100]\n",
    "\n",
    "# subset the dataframe to get only cells which are nearest neighbors \n",
    "group = np.array(metadata[metadata[\"idx\"].isin(top_hundred.tolist()[0])][\"cells\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leiden Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have celltype annotations we might simply use Leiden Clustering to create cell aggregates. We would expect these to be celltype-specific if done at high enough resolution, but we could also try to do Leiden Clustering based on each celltype individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import leidenalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a resolution of 100, I get 930 clusters, which is probably too much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata, resolution=100, key_added=\"cluster_pvi\")#, restrict_to=(\"celltype\", adata.obs.celltype.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.cluster_pvi.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.obs.cluster_pvi, bins = 500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I reduce the resolution to 50, I get 483 cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata, resolution=50, key_added=\"cluster_pvi\")#, restrict_to=(\"celltype\", adata.obs.celltype.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.cluster_pvi.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.obs.cluster_pvi, bins = 500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.cluster_pvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.groupby(\"cluster_pvi\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset to do Leiden Clustering on individual celltypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it make sense to create clusters of different size or should I aim to create clusters which are similar in size for each celltype. This means that if there are more cells of a celltype I will get more clusters of that type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erythroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset = adata[adata.obs['celltype'] == \"Erythroid2\"]\n",
    "print(f\"The number of celsl for Erythroids are: {adata_subset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata_subset, resolution=.6, key_added=\"Erythroid_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata_subset, min_dist=0.2)\n",
    "sc.pl.umap(adata_subset, color='Erythroid_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset.obs.groupby(\"Erythroid_clusters\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erythroid1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset = adata[adata.obs['celltype'] == \"Erythroid1\"]\n",
    "print(f\"The number of celsl for Erythroids are: {adata_subset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata_subset, resolution=.6, key_added=\"Erythroid_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata_subset, min_dist=0.2)\n",
    "sc.pl.umap(adata_subset, color='Erythroid_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset.obs.groupby(\"Erythroid_clusters\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will remove Cluster 6, because it only contains 6 cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erythroid 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset = adata[adata.obs['celltype'] == \"Erythroid3\"]\n",
    "print(f\"The number of celsl for Erythroids are: {adata_subset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata_subset, resolution=.2, key_added=\"Erythroid_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata_subset, min_dist=0.2)\n",
    "sc.pl.umap(adata_subset, color='Erythroid_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset.obs.groupby(\"Erythroid_clusters\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parietal Endoderm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset = adata[adata.obs['celltype'] == \"Parietal_endoderm\"]\n",
    "print(f\"The number of celsl for Erythroids are: {adata_subset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(adata_subset, resolution=.6, key_added=\"clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata_subset, min_dist=0.2)\n",
    "sc.pl.umap(adata_subset, color='clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset.obs.groupby(\"clusters\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to remove cluster 3 and cluster 4, because they contain very few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.celltype.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manus version of doing celltype-specific Leiden clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leiden_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_type in adata.obs['celltype'].unique():\n",
    "    adata_celltype =adata[adata.obs['celltype']==cell_type,:]\n",
    "    if adata_celltype.shape[0]>80:\n",
    "        sc.pp.neighbors(adata_celltype, use_rep=\"X_scVI\",n_neighbors=100, n_pcs=50)\n",
    "        sc.tl.leiden(adata_celltype, resolution=1)\n",
    "        #sc.tl.leiden(adata_celltype, resolution=0.2)  # decreasing resolution\n",
    "        sc.tl.umap(adata_celltype, spread=1., min_dist=.5, random_state=11)\n",
    "        sc.pl.umap(adata_celltype, color=\"leiden\", legend_loc=\"on data\",edges=False,title=cell_type)\n",
    "        adata_celltype.obs['leiden_name'] = [str(s) + '_'+ cell_type for s in adata_celltype.obs['leiden'] ]\n",
    "        adata_celltype.obs['cell_name'] = adata_celltype.obs.index\n",
    "\n",
    "        cluster_celltype = adata_celltype.obs[['cell_name','leiden_name']]\n",
    "        df_leiden_list.append(cluster_celltype)\n",
    "        \n",
    "    else:\n",
    "        adata_celltype.obs['leiden_name'] = [str(0) + '_'+ cell_type for s in range(adata_celltype.obs.shape[0]) ]\n",
    "        adata_celltype.obs['cell_name'] = adata_celltype.obs.index\n",
    "\n",
    "        cluster_celltype = adata_celltype.obs[['cell_name','leiden_name']]\n",
    "        df_leiden_list.append(cluster_celltype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher resolution values lead to more clusters\n",
    "help(sc.tl.leiden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.find_partition(latent, la.ModularityVertexPartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(la.find_partition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi2",
   "language": "python",
   "name": "scvi2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
